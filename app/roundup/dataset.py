#!/usr/bin/env python

'''
# CONCEPTS YOU NEED TO KNOW TO UNDERSTAND THIS CODE

- Dataset: ds, the id of a dataset.  also happens to be a directory path, but that is an implementation detail.
- Sources: a dir containing all the files needed to create the genomes and the metadata about those genomes: gene names, go terms, gene descriptions, genome names, etc.
- Genomes: A dir containing dirs named after each genome and containing a fasta file and blast indexes named after the genome.
  a genome id is an uniprot species identifier.  http://ca.expasy.org/sprot/userman.html#ID_line
- Jobs: a dir containing one dir for each job.  a job is the computational unit used for parallelism.  a job is assigned pairs to run rsd on.
  it runs rsd and stores the orthologs in the orthologs dir in a file named for the job.
- Orthologs: a dir containing files of orthologs generated by rsd.  
- Pair: a tuple of a pair of genomes, the "query" genome and "subject" genome.  a well-formed pair is sorted.
- Metadata: caches information about a dataset for quick retrieval.  
- Dones: track which stages of creating a dataset or running a computation are done.
- Stats: track the time it takes for rsd and the forward and reverse blast hits precomputation to run.
- Download: a dir containing zipped files of orthologs and genomes, for bulk download.
'''

# taxon used for genome b/c orgCode can be used for multiple organisms (e.g. subspecies).
# orgName used for genomeName

# A description of Uniprot Knowledgebase dat files:
# http://web.expasy.org/docs/userman.html
# "mnemonic species identification code"

# A list of all species ids abbreviations, their kingdom, taxon id, and official names, common names, and synonyms.  Cool!
# http://www.uniprot.org/docs/speclist
# "organism (species) identification code", "organism code"

# A description of how the uniprot taxonomy is organized, with explanation of organism codes.
# "mnemonic organism identification code"
# http://www.uniprot.org/help/taxonomy


# standard library modules
import argparse
import collections
import contextlib
import datetime
import glob
import itertools
import json
import logging
import os
import random
import re
import shutil
import subprocess
import sys
import time

# Add the root python dir to sys.path, so imports will work when dataset.py
# is run as a script from the command line.
# /root/module/dir/roundup/dataset.py -> /root/module/dir
_root_module_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if _root_module_dir not in sys.path:
    sys.path.append(_root_module_dir)

# our modules
import config
import dones
import fasta
import kvstore
import lsfdo
import nested
import orthoxml
import orthutil
import roundup_common
import rsd
import uniprot
import util


DEFAULT_NUM_JOBS = 4000 # the default number of jobs used to compute orthologs
MIN_GENOME_SIZE = 200 # ignore genomes with fewer sequences
DIR_MODE = 0775 # directories in this dataset are world readable and group writable.

# keys used for termToData and taxonToData  
NAME = 'name' # short to save space. boo.
TYPE = 'type' # short key to save space. boo
CAT_NAME = 'cat_name'
CAT_CODE = 'cat_code'

# METADATA STORES, keys for getData()
DATASET = 'dataset'
GENES = 'genes' 
GENE_TO_GENOME = 'gene_to_genome'
GENE_TO_NAME = 'gene_to_name'
GENE_TO_DESC = 'gene_to_desc'
GENE_TO_GO_TERMS = 'gene_to_go_terms'
GENE_TO_GENE_IDS = 'gene_to_gene_ids'
TERM_TO_DATA = 'term_to_data'
GENOME_TO_GENES = 'genome_to_genes'
TAXON_TO_DATA = 'taxon_to_data'
BLAST_STATS = 'blast_stats'
RSD_STATS = 'rsd_stats'
CHANGE_LOG = 'change_log'
BIG_DATA_KEYS = [GENES, GENE_TO_GENOME, GENE_TO_NAME, GENE_TO_DESC,
        GENE_TO_GO_TERMS, GENE_TO_GENE_IDS, TERM_TO_DATA, GENOME_TO_GENES,
        TAXON_TO_DATA, BLAST_STATS, RSD_STATS, CHANGE_LOG]

# For performance, use local disk during computation.
LOCAL_DIR = '/tmp'


##################
# DATASET CREATION

def prepare_dataset(ds):
    '''
    Make the directory structure, tables, etc., a new dataset needs.
    '''
    for path in [ds, getGenomesDir(ds), getOrthologsDir(ds), getJobsDir(ds),
                 getSourcesDir(ds), getDownloadDir(ds)]:
        if not os.path.exists(path):
            os.makedirs(path, DIR_MODE)


########################################
# DOWNLOADING AND PROCESSING SOURCE DATA


def download_taxon_sources(ds):
    sourcesDir = getSourcesDir(ds)
    urlDests = [('ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxcat.tar.gz', os.path.join(sourcesDir, 'ncbi', 'taxcat.tar.gz')),
                ('ftp://ftp.ncbi.nih.gov/pub/taxonomy/taxdump.tar.gz', os.path.join(sourcesDir, 'ncbi', 'taxdump.tar.gz'))]
    for url, dest in urlDests:
        downloadSource(ds, url, dest)

    sources = [url for url, dest in urlDests]
    setData(ds, 'taxon_sources', sources)


def download_go_sources(ds):
    sourcesDir = getSourcesDir(ds)
    urlDests = [('ftp://ftp.geneontology.org/pub/go/godatabase/archive/go_daily-termdb-tables.tar.gz',
                 os.path.join(sourcesDir, 'geneontology/go_daily-termdb-tables.tar.gz'))]
    for url, dest in urlDests:
        downloadSource(ds, url, dest)

    sources = [url for url, dest in urlDests]
    setData(ds, 'go_sources', sources)


def download_uniprot_sources(ds):
    sourcesDir = getSourcesDir(ds)
    currentUrl = 'ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete'
    uniprotFiles = ['reldate.txt', 'uniprot_sprot.dat.gz', 'uniprot_trembl.dat.gz']
    urlDests = [(currentUrl + '/' + f, os.path.join(sourcesDir, 'uniprot', f)) for f in uniprotFiles]
    for url, dest in urlDests:
        downloadSource(ds, url, dest)

    # since Uniprot does not concurrently release an "archive" url when
    # they put out a new release, pretend like the uniprot source was the
    # "archive" url, not the "current" urls.
    # e.g. release = 2011_09
    release = uniprot.parseRelease(os.path.join(sourcesDir, 'uniprot', 'reldate.txt'))
    uniprotArchiveUrl = 'ftp://ftp.uniprot.org/pub/databases/uniprot/previous_releases/release-{0}/knowledgebase/knowledgebase{0}.tar.gz'.format(release)
    sources = [uniprotArchiveUrl]
    setData(ds, 'uniprot_sources', sources)
    setData(ds, 'uniprotRelease', release)


def download_sources(ds):
    '''
    Download uniprot files contained in an archive of a uniprot release.
    Download ncbi taxon categories file.
    Download go database.
    Save source urls, but fudge the uniprot ones to look like the archive url, b/c the "current" urls
    are not stable.
    '''

    print 'download_sources: {}'.format(ds)
    download_taxon_sources(ds)
    download_go_sources(ds)
    download_uniprot_sources(ds)

    sources = getData(ds, 'uniprot_sources') + getData(ds, 'taxon_sources') + getData(ds, 'go_sources') 
    print sources
    setData(ds, 'sources', sources)
    print 'done downloading sources.'


def downloadSource(ds, url, dest):
    '''
    helper function.  downloads a source and marks it complete
    '''
    if not os.path.exists(os.path.dirname(dest)):
        os.makedirs(os.path.dirname(dest), DIR_MODE)
    print 'downloading {} to {}...'.format(url, dest)
    if getDones(ds).done(('download', url)):
        print '...skipping because already downloaded.'
        return
    cmd = 'curl --remote-time --output '+dest+' '+url
    subprocess.check_call(cmd, shell=True)
    print
    getDones(ds).mark(('download', url))
    print '...done.'
    pause = 10
    print 'pausing for {} seconds.'.format(pause)
    time.sleep(pause)


def process_taxon_sources(ds):
    files = ['ncbi/taxcat.tar.gz', 'ncbi/taxdump.tar.gz']
    for f in files:
        process_source(ds, f)


def process_go_sources(ds):
    files = ['geneontology/go_daily-termdb-tables.tar.gz']
    for f in files:
        process_source(ds, f)


def process_uniprot_sources(ds):
    files = [
             'uniprot/uniprot_sprot.dat.gz', 
             'uniprot/uniprot_trembl.dat.gz', 
             ]
    for f in files:
        process_source(ds, f)


def process_sources(ds):
    '''
    gunzip (and untar) some source files.
    '''
    print 'processing sources...'
    process_taxon_sources(ds)
    process_go_sources(ds)
    process_uniprot_sources(ds)
    print '...done'


def process_source(ds, filename):
    sourcesDir = getSourcesDir(ds)
    path = os.path.join(sourcesDir, filename)
    if getDones(ds).done(('process source', path)):
        print '...skipping processing {} because already done.'.format(path)
        return
    print 'processing', path
    if path.endswith('.tar.gz'):
        print '...tar xzf file'
        subprocess.check_call(['tar', '-xzf', path], cwd=os.path.dirname(path))
    elif path.endswith('.gz') and os.path.exists(path):
        print '...gunzip file'
        subprocess.check_call(['gunzip', path])
    getDones(ds).mark(('process source', path))


######################################
# EXTRACT TAXON AND GENE ONTOLOGY DATA


def extract_taxon_data(ds):
    '''
    taxon category codes and names from ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxcat_readme.txt:
    A = Archaea
    B = Bacteria
    E = Eukaryota
    V = Viruses and Viroids
    U = Unclassified and Other
    '''
    sourcesDir = getSourcesDir(ds)
    catCodeToName = {'A': 'Archaea', 'B': 'Bacteria', 'E': 'Eukaryota', 'V': 'Viruses and Viroids', 'U': 'Unclassified and Other'}
    taxonToData = collections.defaultdict(dict)
    for line in open(os.path.join(sourcesDir, 'ncbi/categories.dmp')):
        cat, speciestaxon, taxon = line.split()
        taxonToData[taxon][CAT_CODE] = cat
        taxonToData[taxon][CAT_NAME] = catCodeToName[cat]
    for line in open(os.path.join(sourcesDir, 'ncbi/names.dmp')):
        splits = [s.strip() for s in line.split('|')]
        if not len(splits) == 5:
            raise Exception('Wrong number of fields', splits, line)
        taxon, name, uniqueName, nameClass, other = splits
        if nameClass == 'scientific name':
            taxonToData[taxon][NAME] = name
    setData(ds, TAXON_TO_DATA, taxonToData)

        
def extract_gene_ontology_data(ds):
    '''
    creates a lookup for go term accessions to name and term_type.  e.g. 'GO:2000779' => 'regulation of double-strand break repair', 'biological_process'    
    '''
    # `id` int(11) NOT NULL AUTO_INCREMENT,
    # `name` varchar(255) NOT NULL DEFAULT '',
    # `term_type` varchar(55) NOT NULL,
    # `acc` varchar(255) NOT NULL,
    # `is_obsolete` int(11) NOT NULL DEFAULT '0',
    # `is_root` int(11) NOT NULL DEFAULT '0',
    # `is_relation` int(11) NOT NULL DEFAULT '0',

    termToData = {}
    with open(os.path.join(getSourcesDir(ds), 'geneontology/go_daily-termdb-tables/term.txt')) as fh:
        for line in fh:
            id, name, termType, acc, isObsolete, etc = line.strip().split('\t', 5)
            if isObsolete == '0' and termType == 'biological_process':
                termToData[acc] = {NAME: name, TYPE: termType}
    setData(ds, TERM_TO_DATA, termToData)


###########################################################
# EXTRACT FASTA FILES AND OTHER DATA FROM UNIPROT DAT FILES


def examine_dats(ds, dats=None, surprisesFile=None, countsFile=None):
    '''
    Investigate which genomes we should include in roundup.  Gather data about
    the sequences and genomes in the dat files, primarily counts per genome of
    total sequences, complete sequences, reference sequences, seqs in sprot,
    seqs in trembl.  Check for surprises, like having >1 taxon or genome name
    or organism code per genome.  This function is used to better understand
    the dat files, to decide whether or not to just use 'Complete proteome'
    seqs, and to see which genomes are new (not in last roundup).

    ds: a dataset (dir)
    dats: a list of dat file paths.  Defaults to the dat files in the dataset.
    surprisesFile: where to write all the ways the dat files surprised the
    parser by being formed in unexpected ways.
    countsFile: where to write the count of sequences for each genome,
    including total, complete, and reference counts.
    '''
    if not dats:
        dats = getDats(ds)
    if not surprisesFile:
        surprisesFile = os.path.join(ds, 'dat_surprises.txt')
    if not countsFile:
        countsFile = os.path.join(ds, 'dat_genome_counts.txt')
    print 'dats:', dats
    print 'surprisesFile:', surprisesFile
    print 'countsFile:', countsFile

    genomeToCount = collections.defaultdict(int) # maps each genome to the number of sequences (in the dat files) for that genome.
    genomeToCompleteCount = collections.defaultdict(int) # track number of seqs in the Complete proteome set of an organism
    genomeToReferenceCount = collections.defaultdict(int) # track number of seqs in the Reference proteome set of an organism
    genomeToSprotCount = collections.defaultdict(int) # track number of seqs from Swissprot
    genomeToTremblCount = collections.defaultdict(int) # track number of seqs from Trembl
    genomeToNames = collections.defaultdict(set)
    genomeToTaxons = collections.defaultdict(set)
    genomeToOrgCodes = collections.defaultdict(set) # maps each genome to a uniprot organism (species) id code. e.g. ECOLI, HUMAN, XENTR

    # collect exceptions to my assumptions about dat files
    # are genes or genomes missing something they should have?
    # do genes have more than one gene name, description?
    # do genomes have more than one version of a name, a taxon, a complete status?
    allSurprises = []

    for path in dats:
        for entryNum, data in enumerate(uniprot.genDatEntries(path)):
            ns, gene, orgCode, orgName, taxon, geneName, geneDesc, complete, reference, geneIds, goTerms, fastaLines, surprises = data
            allSurprises.extend(surprises)

            # taxon used for genome b/c orgCode can be used for multiple organisms (e.g. subspecies).
            # orgName used for genomeName
            assert taxon
            genome = taxon

            # Ignore "9" org code seqs b/c they are not a species or subspecies. http://www.uniprot.org/help/taxonomy
            if orgCode[0] == '9':
                continue

            genomeToOrgCodes[genome].add(orgCode)
            genomeToNames[genome].add(orgName)
            genomeToTaxons[genome].add(taxon)
            genomeToCount[genome] += 1
            if complete:
                genomeToCompleteCount[genome] += 1
            if reference:
                genomeToReferenceCount[genome] += 1
            if ns == 'sp':
                genomeToSprotCount[genome] += 1
            if ns == 'tr':
                genomeToTremblCount[genome] += 1

    countList = []
    countData = {}
    for genome in genomeToCount:
        # surprise! genome has multiple org codes
        if len(genomeToOrgCodes[genome]) > 1:
            allSurprises.append('\t'.join(('multiple_genome_org_codes', genome, ' '.join(genomeToOrgCodes[genome]))))
        # surprise! genome has multiple names
        if len(genomeToNames[genome]) > 1:
            allSurprises.append('\t'.join(('multiple_genome_names', genome, ' '.join(genomeToNames[genome])))) 
        # surprise! genome has multiple taxons
        if len(genomeToTaxons[genome]) > 1:
            allSurprises.append('\t'.join(('multiple_genome_taxons', genome, ' '.join(genomeToTaxons[genome])))) 

        refCount = genomeToReferenceCount[genome]
        compCount = genomeToCompleteCount[genome]
        if refCount and refCount != compCount:
            # surprise! different number of seqs in reference proteome than in complete proteome
            allSurprises.append('\t'.join(('reference_count_with_different_complete_count', '', genome, '', '', 
                                            'reference count: {}, complete count: {}'.format(refCount, compCount))))
        data = {'genome': genome, 'name': list(genomeToNames[genome])[0], 'taxon': list(genomeToTaxons[genome])[0],
                'org_code': list(genomeToOrgCodes[genome])[0],
                'count': genomeToCount[genome], 'complete_count': compCount, 'reference_count': refCount,
                'sprot_count': genomeToSprotCount[genome], 'trembl_count': genomeToTremblCount[genome],
                'num_names': len(genomeToNames[genome]), 'num_taxons': len(genomeToTaxons[genome]),
                'num_org_codes': len(genomeToOrgCodes[genome])}
        countData[genome] = data
        countList.append([data['count'], data['complete_count'], data['reference_count'], data['sprot_count'], data['trembl_count'],
                          data['genome'], data['org_code'], data['name'], data['num_names'], data['num_taxons'], data['num_org_codes']])

    with open(countsFile, 'w') as fh:
        for data in sorted(countList):
            fh.write('\t'.join(str(d) for d in data) + '\n')

    with open(surprisesFile, 'w') as fh:
        for surprise in allSurprises:
            fh.write(surprise + '\n')

    setData(ds, 'dat_surprises', allSurprises)
    setData(ds, 'dat_genome_counts', countData)

    print 'all done.', datetime.datetime.now()


def set_genomes_from_filter(ds):
    '''
    Set the genomes to be ones that:

    - have >= MIN_GENOME_SIZE sequences marked 'Complete proteome'
    - and are archaea, bacteria, or eukayota.  Reject viruses and unknown taxons.
    - and have taxon data

    Report all genomes rejected for missing taxon data if they meet the
    sequence count criterion.
    '''
    countData = getData(ds, 'dat_genome_counts')
    taxonToData = getTaxonToData(ds)
    genomes = [g for g in countData]
    print 'all_genomes', len(genomes)
    # filter out small genomes
    genomes1 = [g for g in genomes if countData[g]['complete_count'] >= MIN_GENOME_SIZE]
    print 'after_size_filter', len(genomes1)
    # filter out genomes missing taxon data
    genomes2 = []
    for genome in genomes1:
        if genome not in taxonToData:
            print 'missing_taxon', genome, 'complete_count', countData[genome]['complete_count'], 'name', countData[genome]['name']
        else:
            genomes2.append(genome)
    print 'after_missing_taxon_filter', len(genomes2)
    # filter out genomes that are viruses or unclassified
    genomes3 = []
    for genome in genomes2:
        catCode = taxonToData[genome].get(CAT_CODE)
        if catCode in ['A', 'B', 'E']:
            genomes3.append(genome)
        else:
            print 'filtered_taxon_cat_code', genome, 'cat_code', catCode, 'complete_count', countData[genome]['complete_count'], 'name', countData[genome]['name']
    print 'after_taxon_category_filter', len(genomes3)
    setGenomes(ds, genomes3)


def make_genome_to_name(ds):
    '''
    Build genomeToName from genome and taxon data and set it.
    '''
    genomes = getGenomes(ds)
    taxonToData = getTaxonToData(ds)
    genomeToName = {g: taxonToData[g][NAME] for g in genomes} # use taxon name as genome name.

    # paranoid check: All genome names should be unique
    assert len(genomeToName.values()) == len(set(genomeToName.values()))

    setGenomeToName(ds, genomeToName)


def make_genome_to_taxon(ds):
    '''
    Build genomeToTaxon from genome data and set it.
    '''
    genomes = getGenomes(ds)
    # Easy since genome (ids) are ncbi taxon ids.
    genomeToTaxon = {g: g for g in genomes}
    setGenomeToTaxon(ds, genomeToTaxon)


def extract_from_dats(ds, dats=None, writing=True, cleanDirs=False, bufSize=5000000):
    '''
    Gather data about each 'Complete proteome' sequence, including name, description, go terms, ncbi gene ids.  Gather data about each
    associated genome, including name, ncbi taxon id, and sequence counts.  Create fasta files containing the seqs, one for each genome,
    trying to mimic the fasta name lines in the fasta files from uniprot.

    ds: a dataset (dir)
    dats: a list of dat file paths.  Defaults to the dat files in the dataset.
    writing: debugging. if False, fasta files will not be writen. Useful for debugging
    cleanDirs: optimization.  if True, all fasta files in the dataset will be removed before splitting the genomes, which takes time.
    bufSize: number of fasta sequence to cache in memory before writing to files.  this is to avoid writing frequently to
      files, b/c opening and closing files on Isilon fileserver is slow.
    '''

    if not dats:
        dats = getDats(ds)
    print 'dats:', dats
            
    # remove any pre-existing genomes.
    if cleanDirs:
        print 'cleaning genomes directory...', datetime.datetime.now()
        cleanGenomes(ds)

    # helper function to write fasta lines to a genome file.
    writeGenomes = set() # track which genomes we have already seen
    def writeToGenome(genome, data):
        ''' data: a list of fasta lines, including newlines '''
        if not writing: # speed performance when not testing behavior that does not involve writing fasta
            return        
        if genome not in writeGenomes: # first time writing to the genome
            makeGenomeDir(ds, genome) # make genome dir if missing
            mode = 'w'
            writeGenomes.add(genome)
        else:
            mode = 'a'
        with open(getGenomeFastaPath(ds, genome), mode) as outfh:
            output = ''.join(data)
            outfh.write(output)

    # use count data to avoid rescanning dat files.
    countData = getData(ds, 'dat_genome_counts')
    genomes = getGenomes(ds)
    genomeToCount = {g: countData[g]['complete_count'] for g in genomes}
    genomeToOrgCode = {g: countData[g]['org_code'] for g in genomes}

    # paranoid check:  No genome should have >1 name, taxon, or org code.
    for g in genomes:
        assert countData[g]['num_names'] == 1 and countData[g]['num_taxons'] == 1 and countData[g]['num_org_codes'] == 1

    geneToGenome = {} # track which sequences belong to which genome.  store sequences of all complete genomes
    geneToName = {}
    geneToDesc = {}
    geneToGeneIds = {}
    geneToGoTerms = {}
    genomeToGenes = collections.defaultdict(list)
    genomeToLines = collections.defaultdict(list) # buffer fasta sequence lines for output

    # Pass 2: collect data about genes and genomes, but only for genomes that are not too small.
    for path in dats:
        for entryNum, data in enumerate(uniprot.genDatEntries(path)):
            ns, gene, orgCode, orgName, taxon, geneName, geneDesc, complete, reference, geneIds, goTerms, fastaLines, surprises = data

            # taxon used for genome b/c orgCode can be used for multiple organisms (e.g. subspecies).
            genome = taxon

            # Ignore "9" org code seqs b/c they are not a species or subspecies. http://www.uniprot.org/help/taxonomy
            # Ignore seqs not marked "Complete proteome"
            # Ignore seqs from too small genomes
            if orgCode[0] == '9' or not complete or genome not in genomeToCount:
                continue
            
            geneToGenome[gene] = genome
            geneToName[gene] = geneName
            geneToDesc[gene] = geneDesc
            geneToGeneIds[gene] = geneIds
            geneToGoTerms[gene] = goTerms
            genomeToGenes[genome].append(gene)
            genomeToLines[genome].extend(fastaLines)

            # collect fasta lines for writing to fasta files
            if entryNum % bufSize == 0:
                # write out collected lines to the various genome fasta files
                print 'writing collected lines to fasta files for {} genomes...'.format(len(genomeToLines)), datetime.datetime.now()
                for genome in genomeToLines:
                    writeToGenome(genome, genomeToLines[genome])
                genomeToLines.clear()
                print 'collecting more lines...', datetime.datetime.now()

        # done with path.
        # write out collected lines to the various genome fasta files
        print 'finishing writing collected lines to fasta files for {} genomes...'.format(len(genomeToLines)), datetime.datetime.now()
        for genome in genomeToLines:
            writeToGenome(genome, genomeToLines[genome])
        genomeToLines.clear()
        print 'done writing collected lines.', datetime.datetime.now()

    print 'updating metadata...', datetime.datetime.now()
    setData(ds, GENE_TO_GO_TERMS, geneToGoTerms)
    setData(ds, GENE_TO_GENE_IDS, geneToGeneIds)
    setData(ds, GENE_TO_NAME, geneToName)
    setData(ds, GENE_TO_DESC, geneToDesc)
    setData(ds, GENE_TO_GENOME, geneToGenome)
    setData(ds, GENOME_TO_GENES, genomeToGenes)
    setGenomes(ds)
    setData(ds, 'genomeToCount', genomeToCount)
    setData(ds, 'genomeToOrgCode', genomeToOrgCode)

    print 'all done.', datetime.datetime.now()


def updateGenomeCounts(ds):
    '''
    Update dataset metadata with the sequence count of each genome fasta file.
    '''
    genomes = getGenomes(ds)
    genomeToCount = {}
    for g in genomes:
        genomeToCount[g] = fasta.numSeqsInFastaDb(getGenomeFastaPath(ds, g))
        print g, genomeToCount[g]
    setData(ds, 'genomeToCount', genomeToCount)


def format_genomes(ds):
    '''
    Format genomes for blast, distributing the formatting via LSF.
    ds: dataset for which to format genomes
    '''
    print 'formatting genomes. {}'.format(ds)
    ns = 'roundup_dataset_{}_dones'.format(getDatasetId(ds))
    genomes = getGenomes(ds)
    # Since the Orchestra LSF queues require jobs of a certain length, format a
    # batch of genomes as a single job.
    tasks = [lsfdo.FuncNameTask('format_some_genomes_{}'.format(i),
                                'roundup.dataset.format_some_genomes',
                                [ds, some])
             for i, some in enumerate(util.groupsOfN(genomes, 10))]
    opts = [['-q', 'short', '-W', '60'] for task in tasks]
    lsfdo.bsubmany(ns, tasks, opts)


def format_some_genomes(ds, genomes):
    for genome in genomes:
        format_genome(ds, genome)


def format_genome(ds, genome):
    fastaPath = getGenomeFastaPath(ds, genome)
    print 'format {}'.format(genome)
    rsd.formatForBlast(fastaPath)


#############################################
# COLLECTING SIZE AND PERFORMANCE STATISTICS 

def extract_dataset_stats(ds):
    '''
    update the dataset metadata with num genomes, num pairs, total num orthologs.
    '''
    numGenomes = len(getGenomes(ds))
    numPairs = len(getPairs(ds))
    numOrthologs = 0
    for params, orthologs in orthutil.orthDatasFromFilesGen(getOrthologsFiles(ds)):
        numOrthologs += len(orthologs)
    setData(ds, 'numGenomes', numGenomes)
    setData(ds, 'numPairs', numPairs)
    setData(ds, 'numOrthologs', numOrthologs)


def extract_performance_stats(ds, pairs=None):
    '''
    pairs: default is to collect stats for all pairs.  It can be useful for testing to set pairs to a specific list of pairs.
    times are in seconds.
    '''
    blastStats = {}
    rsdStats = {}
    totalTime = 0
    totalRsdTime = 0
    totalBlastTime = 0

    if pairs is None:
        pairs = getPairs(ds)

    def elapsedTime(stats):
        return stats['endTime'] - stats['startTime']

    with statsCM(ds) as stats:
        for qdb, sdb in pairs:
            forwardTime = elapsedTime(stats.getBlast(qdb, sdb))
            reverseTime = elapsedTime(stats.getBlast(sdb, qdb))
            rsdTime = elapsedTime(stats.getRsd(qdb, sdb))

            blastStats[json.dumps((qdb, sdb))] = forwardTime
            blastStats[json.dumps((sdb, qdb))] = reverseTime
            rsdStats[json.dumps((qdb, sdb))] = rsdTime

            totalTime += forwardTime + reverseTime + rsdTime
            totalBlastTime += forwardTime + reverseTime
            totalRsdTime += rsdTime

    print 'total time:', totalTime
    print 'total blast time:', totalBlastTime
    print 'total rsd time:', totalRsdTime
    print 'saving stats'
    setData(ds, BLAST_STATS, blastStats)
    setData(ds, RSD_STATS, rsdStats)


def make_change_log(ds, others):
    '''
    others: a list of other datasets to compare this one too.
    '''
    dss = [ds] + others
    dsids = [getDatasetId(d) for d in dss]
    data = collections.defaultdict(dict)
    for d, dsid in zip(dss, dsids):
        print d
        genomes = getGenomes(d)
        print 'len(genomes)=', len(genomes)
        genomeToName = getGenomeToName(d)
        genomeToName = {g: genomeToName[g] for g in genomes}
        print 'len(genomeToName)=', len(genomeToName)
        genomeToTaxon = getGenomeToTaxon(d)
        genomeToTaxon = {g: genomeToTaxon[g] for g in genomes}
        print 'len(genomeToTaxon)=', len(genomeToTaxon)
        taxonToGenome = {genomeToTaxon[g]: g for g in genomeToTaxon}
        print 'len(taxonToGenome)=', len(taxonToGenome)
        taxonToName = {t: genomeToName[taxonToGenome[t]] for t in taxonToGenome}
        taxToData = getTaxonToData(d)
        taxToCat = {t: taxToData[t][CAT_CODE] for t in taxToData if CAT_CODE in taxToData[t] }
        taxons = [genomeToTaxon[g] for g in genomes]
        euks = [t for t in taxons if taxToCat.get(t) == 'E']
        assert len(taxons) == len(set(taxons)) == len(genomes) == len(set(genomes))
        taxons = set(taxons)
        euks = set(euks)
        def spec(name):
            return ' '.join(name.split()[:2])
        taxonNames = set(taxonToName[g] for g in taxons)
        eukNames = set([taxonToName[g] for g in euks])
        taxonSpecies = set([spec(n) for n in taxonNames])
        eukSpecies = set(spec(n) for n in eukNames)
        taxonSpeciesToNames = collections.defaultdict(set)
        [taxonSpeciesToNames[spec(g)].add(g) for g in taxonNames]
        multiTaxonSpecies = set([s for s in taxonSpeciesToNames if len(taxonSpeciesToNames[s]) > 1])
        eukSpeciesToNames = collections.defaultdict(set)
        [eukSpeciesToNames[spec(g)].add(g) for g in eukNames]
        multiEukSpecies = set([s for s in eukSpeciesToNames if len(eukSpeciesToNames[s]) > 1])
        data[dsid]['taxons'] = list(taxons) # convert to list b/c json does not serialize sets.  boo.
        data[dsid]['euks'] = list(euks)
        data[dsid]['taxonToName'] = taxonToName
        data[dsid]['taxonNames'] = list(taxonNames)
        data[dsid]['eukNames'] = list(eukNames)
        data[dsid]['taxonSpecies'] = list(taxonSpecies)
        data[dsid]['eukSpecies'] = list(eukSpecies)
        data[dsid]['multiTaxonSpecies'] = list(multiTaxonSpecies)
        data[dsid]['multiEukSpecies'] = list(multiEukSpecies)
        
    descs = 'taxons euks taxonSpecies eukSpecies multiTaxonSpecies multiEukSpecies'.split()
    # differences in taxonNames and eukNames are not interesting because names are not comparable when
    # we use ncbi taxonomy names in one release and uniprot organism names in another.
    diffs = collections.defaultdict(dict)
    for dsid1, dsid2 in itertools.permutations(dsids, 2):
        print 'difference', dsid1, ' - ', dsid2
        descDiffs = {}
        for desc in descs:
            diff = list(set(data[dsid1][desc]) - set(data[dsid2][desc]))
            size = len(diff)
            print desc, 'len({}) - len({}) = {}'.format(dsid1, dsid2, size)
            print '\n'.join(['\t'+str(x) for x in diff])
            descDiffs[desc] = {'difference': diff, 'size': size}
        diffs[dsid1][dsid2] = descDiffs
    sizes = {}
    for dsid in dsids:
        print 'sizes', dsid
        descSizes = {}
        for i, desc in enumerate(descs):
            size = len(data[dsid][desc])
            print desc, 'len({}) = {}'.format(dsid, size)
            descSizes[desc] = size
        sizes[dsid] = descSizes
    for dsid1, dsid2 in itertools.permutations(dsids, 2):
        print 'difference', dsid1, ' - ', dsid2
        diff = list(set(data[dsid1]['euks']) - set(data[dsid2]['euks']))
        print '\n'.join([str(t) + '\t' + str(data[dsid1]['taxonToName'][t]) for t in diff])
    changeLog = {'data': data, 'differences': diffs, 'sizes': sizes}
    setData(ds, CHANGE_LOG, changeLog)


##############################################
# PREPARING GENOMES AND ORTHOLOGS FOR DOWNLOAD

def process_genomes_for_download(ds):
    '''
    copy genome fasta files into a dir under download dir, then tar.gz the dir.
    '''
    dsid = getDatasetId(ds)
    downloadDir = getDownloadDir(ds)
    genomes = getGenomes(ds)
    downloadGenomesDir = os.path.join(downloadDir, 'genomes')
    if not os.path.exists(downloadGenomesDir):
        os.mkdir(downloadGenomesDir, DIR_MODE)
    for genome in genomes:
        print genome
        shutil.copy(getGenomeFastaPath(ds, genome), downloadGenomesDir)

    print 'tarring dir'
    subprocess.check_call(['tar', '-czf', 'roundup-{}-genomes.tar.gz'.format(dsid), 'genomes'], cwd=downloadDir)
    print 'deleting dir'
    shutil.rmtree(downloadGenomesDir)


def getDownloadGenomesPath(ds):
    dsid = getDatasetId(ds)
    return os.path.join(getDownloadDir(ds), 'roundup-{}-genomes.tar.gz'.format(dsid))


def getDownloadOrthologsPath(ds, div, evalue):
    dsid = getDatasetId(ds)
    downloadDir = getDownloadDir(ds)
    return os.path.join(downloadDir, 'roundup-{}-orthologs_{}_{}.txt.gz'.format(dsid, div, evalue))


def collate_orthologs(ds, div_evalues=None):
    '''
    Collate orthologs from jobs files into files in the download dir, one for
    each parameter combination in the job files.

    div_evalues: a list of tuples of (divergence, evalue) thresholds in the
    dataset to collate.  By default (None) all divergence and evalue
    combinations in the dataset are collated.  Example: [('0.8', '1e-5')]
    '''
    div_evalues = div_evalues if div_evalues is not None else roundup_common.divEvalues()
    txt_paths = [txt_download_path(ds, div, evalue) for div, evalue in div_evalues]
    # cache open filehandles
    fh_cache = dict([(divEvalue, open(path, 'w')) for divEvalue, path in
                     zip(div_evalues, txt_paths)])
    try:
        print 'collating orthologs'
        for orthFile in sorted(getOrthologsFiles(ds)):
            logging.debug('collating {} at {}'.format(orthFile, datetime.datetime.now()))
            for orthData in orthutil.orthDatasFromFileGen(orthFile):
                params, orthologs = orthData
                qdb, sdb, div, evalue = params
                orthutil.orthDatasToStream([orthData], fh_cache[(div, evalue)])
    finally:
        for fh in fh_cache.values():
            fh.close()


def zip_download_paths(ds, div_evalues=None):
    '''
    fter orthologs have been collated and converted to orthoxml, zip the files
    '''
    div_evalues = div_evalues if div_evalues is not None else roundup_common.divEvalues()
    txtPaths = [txt_download_path(ds, div, evalue) for div, evalue in div_evalues]
    xmlPaths = [xml_download_path(ds, div, evalue) for div, evalue in div_evalues]
    for path in txtPaths + xmlPaths:
        zipDownloadPath(path)


def zipDownloadPath(path):
        if os.path.exists(path):
            print 'gzipping', path
            subprocess.check_call(['gzip', path])
        elif os.path.exists(path + '.gz'):
            print 'Skipping already gzipped path', path
        else:
            raise Exception('Missing path for gzip', path)


def txt_download_path(ds, div, evalue):
    dsid = getDatasetId(ds)
    downloadDir = getDownloadDir(ds)
    return os.path.join(downloadDir, 'roundup-{}-orthologs_{}_{}.txt'.format(dsid, div, evalue))


def xml_download_path(ds, div, evalue):
    dsid = getDatasetId(ds)
    downloadDir = getDownloadDir(ds)
    return os.path.join(downloadDir, 'roundup-{}-orthologs_{}_{}.xml'.format(dsid, div, evalue))


#####################
# ORTHOXML CONVERSION

def convert_to_orthoxml(ds, origin, origin_version, database, database_version,
                        protLink=None, clean=False, div_evalues=None):
    '''
    origin: e.g. 'roundup'
    origin_version: e.g. getReleaseName(ds)
    database: e.g. 'Uniprot'
    database_version: e.g. getUniprotRelease(ds)
    protLink: e.g. 'http://www.uniprot.org/uniprot/'
    convert collated orthologs files to orthoxml format.
    clean: if True, will delete the xml files if they already exist.
    '''
    div_evalues = div_evalues if div_evalues is not None else roundup_common.divEvalues()
    txtPaths = [txt_download_path(ds, div, evalue) for div, evalue in div_evalues]
    xmlPaths = [xml_download_path(ds, div, evalue) for div, evalue in div_evalues]

    if clean:
        for xmlPath in xmlPaths:
            if os.path.exists(xmlPath):
                os.remove(xmlPath)

    for txtPath, xmlPath in zip(txtPaths, xmlPaths):
        convertTxtToOrthoXML(ds, txtPath, xmlPath, origin, origin_version,
                             database, database_version, protLink)


def convertTxtToOrthoXML(ds, txtPath, xmlPath, origin, originVersion,
                         databaseName, databaseVersion, protLink):
    '''
    WARNING: xml files on the production roundup dataset are too big.  This step should be skipped.
    txtPath: a orthdatas file
    xmlPath: where to write the OrthoXML version of txtPath
    '''
    print txtPath, '=>', xmlPath
    with open(xmlPath, 'w') as xmlOut:
        convertOrthDatasToXml(
            ds, orthutil.orthDatasFromFileGen(txtPath),
            orthutil.orthDatasFromFileGen(txtPath), xmlOut, origin,
            originVersion, databaseName, databaseVersion, protLink)


def makeOrthoxmlSpecies(genomeToGenes, genomeToName, genomeToTaxon,
                        databaseName, databaseVersion, protLink):
    '''
    Create a orthoxml.Species object for each genome in genomeToGenes,
    generating document-unique ids for each gene. Return a list of
    orthoxml.Species objects, and a dict from gene to the integer id used as a
    gene id for the genes in the Species list.
    '''
    speciesList = []
    geneToIden = {}
    # orthoxml wants a interger gene id for each gene, internal to the xml
    # document.
    iden = 0
    for genome in genomeToGenes:
        genes = []
        for gene in genomeToGenes[genome]:
            iden += 1
            genes.append(orthoxml.Gene(iden, protId=gene))
            geneToIden[gene] = iden
        database = orthoxml.Database(databaseName, databaseVersion,
                                     genes=genes, protLink=protLink)
        species = orthoxml.Species(genomeToName[genome], genomeToTaxon[genome],
                                   [database])
        speciesList.append(species)
    return speciesList, geneToIden


def convertOrthDatasToXml(ds, orthDatas, orthDatasAgain, xmlOut,
        origin='roundup', originVersion=None, databaseName='Uniprot',
        databaseVersion=None, protLink="http://www.uniprot.org/uniprot/"):
    '''
    orthDatas: iter of orthDatas
    orthDatasAgain: another iter of the same orthDatas.  
    xmlOut: a stream in which to write serialized orthoxml.
    a 2-pass method for converting orthdatas for a dataset to orthoxml
    pass through orthologs, collecting genome to gene.
    pass through again, writing out species, scores, groups.
    add parameters to orthoxml notes.
    Done this way to avoid needing to have all orthDatas in memory, in case there are millions of them.
    '''
    if originVersion is None:
        originVersion = getReleaseName(ds)

    if databaseVersion is None:
        databaseVersion = getUniprotRelease(ds)

    print 'getting metadata'
    genomeToName = getGenomeToName(ds)
    genomeToTaxon = getGenomeToTaxon(ds)

    print 'pass 1: collecting genes'
    genomeToGenes = collections.defaultdict(set)
    for params, orthologs in orthDatas:
        qdb, sdb, div, evalue = params
        for qid, sid, dist in orthologs:
            genomeToGenes[qdb].add(qid)
            genomeToGenes[sdb].add(sid)

    print 'making species'
    speciesList, geneToIden = makeOrthoxmlSpecies(genomeToGenes, genomeToName,
            genomeToTaxon, databaseName, databaseVersion, protLink)

    scoreDef = orthoxml.ScoreDef(
        'dist', 'Maximum likelihood evolutionary distance')

    print 'pass 2: writing xml'

    def groupGen():
        for params, orthologs in orthDatasAgain:
            qdb, sdb, div, evalue = params
            for qid, sid, dist in orthologs:
                qGeneRef = orthoxml.GeneRef(geneToIden[qid])
                sGeneRef = orthoxml.GeneRef(geneToIden[sid])
                group = orthoxml.OrthologGroup(
                    [qGeneRef, sGeneRef],
                    scores=[orthoxml.Score(scoreDef.id, dist)])
                yield group

    notes = orthoxml.Notes('These orthologs were computed using the following Reciprocal Smallest Distance (RSD) parameters: divergence={} and evalue={}.  See http://roundup.hms.harvard.edu for more information about Roundup and RSD.'.format(div, evalue))
    for xmlText in orthoxml.toOrthoXML(origin, originVersion, speciesList,
                                       groupGen(), scoreDefs=[scoreDef],
                                       notes=notes):
        xmlOut.write(xmlText)


def convertOrthGroupsToXml(ds, groups, genomeToGenes, div, evalue, xmlOut,
        origin='roundup', originVersion=None, databaseName='Uniprot',
        databaseVersion=None, protLink="http://www.uniprot.org/uniprot/"):
    '''
    ds: used to lookup genome names, taxons, roundup dataset release, and uniprot release.
    groups: a iterable of tuples of (genes, avgDist) for a group of orthologous genes.  avgDist is the mean distance of all orthologous pairs in the group.
    genomeToGenes: a dict from genome to the genes in that genome.
    div: the divergence threshold used to compute the orthologs in groups
    evalue: the evalue threshold used to compute the orthologs in groups
    xmlOut: a stream (e.g. filehandle) that the orthoxml is written to.
    '''
    if originVersion is None:
        originVersion = getReleaseName(ds)

    if databaseVersion is None:
        databaseVersion = getUniprotRelease(ds)

    print 'getting metadata'
    genomeToName = getGenomeToName(ds)
    genomeToTaxon = getGenomeToTaxon(ds)

    print 'making species'
    speciesList, geneToIden = makeOrthoxmlSpecies(genomeToGenes, genomeToName,
            genomeToTaxon, databaseName, databaseVersion, protLink)

    scoreDef = orthoxml.ScoreDef('avgdist', 'Mean maximum likelihood evolutionary distance of all orthologous pairs in a group')

    print 'writing xml'

    def groupGen():
        for genes, avgDist in groups:
            geneRefs = [orthoxml.GeneRef(geneToIden[gene]) for gene in genes]
            score = orthoxml.Score(scoreDef.id, str(avgDist))
            group = orthoxml.OrthologGroup(geneRefs, scores=[score])
            yield group

    notes = orthoxml.Notes('These orthologs were computed using the following Reciprocal Smallest Distance (RSD) parameters: divergence={} and evalue={}.  See http://roundup.hms.harvard.edu for more information about Roundup and RSD.'.format(div, evalue))
    for xmlText in orthoxml.toOrthoXML(origin, originVersion, speciesList, groupGen(), scoreDefs=[scoreDef], notes=notes):
        xmlOut.write(xmlText)


###############
# DATASET STUFF
###############


def getDatasetId(ds):
    '''
    e.g. 3
    '''
    return os.path.basename(ds)


def getGenomesDir(ds):
    return os.path.join(ds, 'genomes')


def getJobsDir(ds):
    return os.path.join(ds, 'jobs')


def getOrthologsDir(ds):
    return os.path.join(ds, 'orthologs')


def getSourcesDir(ds):
    return os.path.join(ds, 'sources')


def getDownloadDir(ds):
    return os.path.join(ds, 'download')


def getDats(ds):
    sourcesDir = getSourcesDir(ds)
    return [os.path.join(sourcesDir, 'uniprot', f) for f in ['uniprot_sprot.dat', 'uniprot_trembl.dat']]


###################
# CLEANING FUNCTION
###################



def cleanOrthologs(ds):
    '''
    remove everything in the orthologs dir.  useful for resetting a computation
    '''
    print 'removing orthologs'
    od = getOrthologsDir(ds)
    for path in [os.path.join(od, f) for f in os.listdir(od)]:
        print 'removing {}'.format(path)
        os.remove(path)


def cleanJobs(ds):
    '''
    removes all the jobs in the dataset.  can be useful for clearing out a test computation
    '''
    # remove all job dirs
    print 'removing jobs'
    jobs = refreshJobs(ds)
    for job in jobs:
        path = getJobDir(ds, job)
        if os.path.exists(path):
            print 'removing {}'.format(path)
            shutil.rmtree(path)

    refreshJobs(ds) # reset jobs cache
    print 'resetting dones...'
    getDones(ds).clean()
    print 'resetting stats...'
    getStats(ds).clean()


def cleanGenomes(ds):
    '''
    remove everything in the genomes dir and reset the genomes list.
    '''
    # delete everything
    print 'deleting everything in genomes dir'
    genomesDir = getGenomesDir(ds)
    everything = [os.path.join(genomesDir, x) for x in os.listdir(genomesDir)]
    for path in everything:
        print 'removing {}'.format(path)
        shutil.rmtree(path)
        
    # reset genomes cache
    print 'resetting genomes cache'
    setGenomes(ds)



###############################
# RUN ORTHOLOG COMPUTATION JOBS

def prepare_jobs(ds, numJobs=DEFAULT_NUM_JOBS, pairs=None):
    '''
    ds: dataset to ready to compute pairs
    numJobs: split pairs into this many jobs.  More jobs = shorter jobs, better parallelism.
      Fewer jobs = fewer dirs and files to make isilon run slowly and overload lsf queue.
      Recommendation: <= 10000.
    pairs: If None, compute orthologs for all pairs of genomes.  If not None, only compute these pairs.  useful for testing.  
    '''
    print 'prepare jobs for {}'.format(ds)

    if pairs is None:
        pairs = getPairs(ds)
    print 'pair count:', len(pairs)

    # create up to N jobs for the pairs to be computed.
    # each job contain len(pairs)/N pairs, except if N does not divide len(pairs) evenly, some jobs get an extra pair.
    # e.g. if you had 11 pairs (1,2,3,4,5,6,7,8,9,10,11) and 3 jobs, the pairs would be split
    # into these jobs: (1,2,3,4),(5,6,7,8),(9,10,11)
    # Distribute pairs randomly among jobs so that each job will have about the same running time.
    # Ideally job running time would be explictly balanced, but currently pairs are just assigned randomly.
    random.shuffle(pairs)
    for i, jobPairs in enumerate(util.splitIntoN(pairs, numJobs)):
        if i % 100 == 0:
            print 'preparing job', i
        job = 'job_{}'.format(i)
        print 'job:', job
        print 'len(jobPairs)=', len(jobPairs)
        jobDir = getJobDir(ds, job)
        print 'jobDir:', jobDir
        os.makedirs(getJobDir(ds, job), 0770)
        setJobPairs(ds, job, jobPairs)

    refreshJobs(ds) # refresh the cached metadata



def compute_jobs(ds):
    '''
    Submit all incomplete and non-running jobs to lsf, so they can compute
    their respective pairs.
    '''
    # a job is a name of a directory and a set of genome pairs for ortholog computation.
    jobs = sorted(getJobs(ds))

    # create a "task" to run on lsf for each job
    dsid = getDatasetId(ds)
    ns = 'roundup_dataset_{}_compute_jobs'.format(dsid)
    names = ['roundup_compute_job_{}_{}'.format(dsid, job) for job in jobs]
    opts = [['-R', 'rusage[tmp=500]', '-q', 'long', '-W', '720:0'] for job in
            jobs]
    tasks = [lsfdo.FuncNameTask(name, 'roundup.dataset.compute_job', [ds, job])
             for name, job in zip(names, jobs)]

    lsfdo.bsubmany(ns, tasks, opts, timeout=0)


def compute_job(ds, job):
    '''
    job: identifies which job this is so it knows which pairs to compute.
    computes orthologs for every pair in the job.  merges the orthologs into a
    single file and puts that file in the dataset orthologs dir.
    '''
    pairs = getJobPairs(ds, job)
    jobDir = getJobDir(ds, job)
    print ds, job, pairs, jobDir

    # compute orthologs for pairs
    for pair in pairs:
        if not getDones(ds).done(('pair', pair)):
            orthologsPath = os.path.join(jobDir, '{}_{}.pair.orthologs.txt'.format(*pair))
            print orthologsPath
            computePair(ds, pair, jobDir, orthologsPath)
            getDones(ds).mark(('pair', pair))

    # merge orthologs for pairs into a single file.
    if not getDones(ds).done(('job_ologs_merge', job)):
        jobOrthologsPath = getJobOrthologsPath(ds, job)
        pairsPaths = [os.path.join(jobDir, '{}_{}.pair.orthologs.txt'.format(*pair)) for pair in pairs]
        orthDatasGen = orthutil.orthDatasFromFilesGen(pairsPaths)
        orthutil.orthDatasToFile(orthDatasGen, jobOrthologsPath)
        getDones(ds).mark(('job_ologs_merge', job))
        
    # delete the individual pair olog files
    for pair in pairs:
        orthologsPath = os.path.join(jobDir, '{}_{}.pair.orthologs.txt'.format(*pair))
        if os.path.exists(orthologsPath):
            os.remove(orthologsPath)


def computePair(ds, pair, workingDir, orthologsPath):
    '''
    ds: the roundup dataset.
    pair: find orthologs for this pair of genomes.
    workingDir: where to save blast hits and orthologs as they get completed.
    orthologsPath: where to write the orthologs.
    precompute blast hits for pair, then compute orthologs for pair, then write orthologs to a file and clean up other files.
    '''
    # a pair is complete when it has written its orthologs to a file and cleaned up its other data files.
    queryGenome, subjectGenome = pair
    queryFastaPath = getGenomeFastaPath(ds, queryGenome)
    subjectFastaPath = getGenomeFastaPath(ds, subjectGenome)
    queryIndexPath = getGenomeIndexPath(ds, queryGenome)
    subjectIndexPath = getGenomeIndexPath(ds, subjectGenome)
    forwardHitsPath = os.path.join(workingDir, '{}_{}.forward_hits.pickle'.format(*pair))
    reverseHitsPath = os.path.join(workingDir, '{}_{}.reverse_hits.pickle'.format(*pair))
    maxEvalue = max([float(evalue) for evalue in roundup_common.EVALUES]) # evalues are strings like '1e-5'
    divEvalues = roundup_common.divEvalues()
    with nested.NestedTempDir(dir=LOCAL_DIR) as tmpDir:
        logging.debug('computePair. tmpDir={}'.format(tmpDir))
        if not getDones(ds).done(('blast', queryGenome, subjectGenome)):
            startTime = time.time()
            rsd.computeBlastHits(queryFastaPath, subjectIndexPath, forwardHitsPath, maxEvalue,
                                 workingDir=tmpDir, copyToWorking=True)
            getStats(ds).putBlast(queryGenome, subjectGenome, startTime=startTime, endTime=time.time())
            getDones(ds).mark(('blast', queryGenome, subjectGenome))

        if not getDones(ds).done(('blast', subjectGenome, queryGenome)):
            startTime = time.time()
            rsd.computeBlastHits(subjectFastaPath, queryIndexPath, reverseHitsPath, maxEvalue,
                                 workingDir=tmpDir, copyToWorking=True)
            getStats(ds).putBlast(subjectGenome, queryGenome, startTime=startTime, endTime=time.time())
            getDones(ds).mark(('blast', subjectGenome, queryGenome))

        if not getDones(ds).done(('roundup', pair)):
            startTime = time.time()
            divEvalueToOrthologs = rsd.computeOrthologsUsingSavedHits(queryFastaPath, subjectFastaPath, divEvalues,
                                                                      forwardHitsPath, reverseHitsPath, workingDir=tmpDir)
            orthDatas = [((queryGenome, subjectGenome, div, evalue), orthologs) for (div, evalue), orthologs in divEvalueToOrthologs.items()]
            orthutil.orthDatasToFile(orthDatas, orthologsPath)
            getStats(ds).putRsd(queryGenome, subjectGenome, divEvalues, startTime=startTime, endTime=time.time())
            getDones(ds).mark(('roundup', pair))
    # clean up files
    if os.path.exists(forwardHitsPath):
        os.remove(forwardHitsPath)
    if os.path.exists(reverseHitsPath):
        os.remove(reverseHitsPath)


######
# JOBS
######

def getJobs(ds):
    '''
    Returns: list of (cached) jobs in the dataset or an empty list if
    no jobs have yet been cached.
    '''
    return getData(ds, 'jobs', [])


def refreshJobs(ds):
    '''
    Refreshes the cache of what jobs are in the jobs dir.  The list of jobs
    is cached, b/c the isilon is wicked slow at listing dirs.
    '''
    jobs = os.listdir(getJobsDir(ds))
    setData(ds, 'jobs', jobs)
    return jobs


def getJobPairs(ds, job):
    with open(os.path.join(getJobDir(ds, job), 'job_pairs.json')) as fh:
        return json.load(fh)


def setJobPairs(ds, job, pairs):
    with open(os.path.join(getJobDir(ds, job), 'job_pairs.json'), 'w') as fh:
        json.dump(pairs, fh, indent=0)



def getJobDir(ds, job):
    return os.path.join(getJobsDir(ds), job)


def getJobOrthologsPath(ds, job):
    return os.path.join(getOrthologsDir(ds), '{}.orthologs.txt'.format(job))


def getComputeJobName(ds, job):
    return getDatasetId(ds) + '_' + job


#########
# GENOMES
#########

def getGenomes(ds):
    '''
    gets genomes cached in the metadata.
    returns: list of genomes in the dataset.
    '''
    return getData(ds, 'genomes', [])


def setGenomes(ds, genomes=None):
    '''
    genomes: a list of genomes to cache.  If None, will get the list from the genomes directory.
    caches genomes in the dataset metadata, b/c the isilon is wicked slow at listing dirs.
    also useful for testing.  can make a small dataset.
    '''
    if genomes is None:
        genomes = os.listdir(getGenomesDir(ds))
    return setData(ds, 'genomes', genomes)


def getGenomesAndPaths(ds):
    '''
    returns: a dict mapping every genome in the dataset to its genomePath.
    '''
    genomesAndPaths = {}
    genomesDir = getGenomesDir(ds)
    for genome in getGenomes(ds):
        genomesAndPaths[genome] = os.path.join(genomesDir, genome)
    return genomesAndPaths


def getGenomePath(ds, genome):
    '''
    a genomePath is a directory containing genome fasta files and blast indexes.
    '''
    return os.path.join(getGenomesDir(ds), genome)


def getGenomeFastaPath(ds, genome):
    '''
    location of fasta file for a genome
    '''
    # .faa for fasta files containing amino acids.  http://en.wikipedia.org/wiki/FASTA_format#File_extension
    return os.path.join(getGenomePath(ds, genome), genome+'.faa')


def getGenomeIndexPath(ds, genome):
    '''
    location of blast index files.
    '''
    return os.path.join(getGenomePath(ds, genome), genome+'.faa')


def addGenomeFasta(ds, genome, fastaPath, name=None, taxon=None):
    '''
    Add the fasta file for a genome to the dataset.  This will copy the 
    fasta file into the dataset in the right location, add genome to the
    list of genomes, and optionally add name and taxon to the genomeToName
    and genomeToTaxon tables.
    '''
    genomePath = getGenomePath(ds, genome)
    genomeFastaPath = getGenomeFastaPath(ds, genome)
    # remove old dir (if it exists)
    if os.path.exists(genomePath):
        shutil.rmtree(genomePath)

    # make directory to contain fasta and blast index files.
    makeGenomeDir(ds, genome)

    shutil.copy(fastaPath, genomeFastaPath)

    if name:
        genomeToName = getGenomeToName(ds)
        genomeToName[genome] = name
        setGenomeToName(ds, genomeToName)

    if taxon:
        genomeToTaxon = getGenomeToTaxon(ds)
        genomeToTaxon[genome] = taxon
        setGenomeToTaxon(ds, genomeToTaxon)


def makeGenomeDir(ds, genome):
    '''
    genome: id of a genome.
    make directory for genome if it does not exist.
    '''
    genomePath = getGenomePath(ds, genome)
    if not os.path.exists(genomePath):
        os.mkdir(genomePath, DIR_MODE)


###################
# ORTHOLOGS/RESULTS
###################


def getOrthologsFiles(ds):
    return glob.glob(os.path.join(getOrthologsDir(ds), '*.orthologs.txt'))


#######
# PAIRS
#######

def getPairs(ds, genomes=None):
    '''
    returns: a sorted list of pairs, where every pair is a sorted list of each combination of two genomes.
    '''
    if genomes is None:
        genomes = getGenomes(ds)
    return sorted(set([tuple(sorted((g1, g2))) for g1 in genomes for g2 in genomes if g1 != g2]))


##########
# METADATA
##########
# The metadata of a dataset is persistent information describing the dataset.
# It is stored in files, so is not safe for concurrent writing.
# The data is accessed as a key-value store, using setData and getData or
# the convenience functions below.
# Small bits of data are stored in a common file.
# Large chunks of data (e.g. geneToGoTerms) are stored in their own file
# for speed and memory restrictions.
# Why not store this in a database?
# - To keep it simple, all dataset info (except when running a computation) is
#   kept in the directory.
# - Accessing data, once loaded into memory, is extremely fast, which is 
#   good for some of the bulk access patterns of the dataset.
# - Several tables with millions of rows.
# Why store it in a database?
# - If concurrent access was needed.
# - Faster updates of small portions of the data, e.g. updating the go terms
#   for a single gene (though this never happens in practice.)

def getData(ds, key, default=None):
    '''
    return: the data associated with key or default if key is not cached.
    '''
    if key in BIG_DATA_KEYS: # big data stored in separate files
        return _getDataInFile(ds, key, default)
    else: # small data all stored in one file
        md = _getDataInFile(ds, DATASET, {})
        return md.get(key, default)


def setData(ds, key, data):
    '''
    Persist data, indexing it with key
    '''
    if key in BIG_DATA_KEYS:
        _setDataInFile(ds, key, data)
    else: # small data all stored in one file
        md = _getDataInFile(ds, DATASET, {})
        md[key] = data
        _setDataInFile(ds, DATASET, md)
    return data


def _getDataInFile(ds, key, default=None):
    '''
    Deserialize and return the data cached in the file indexed by key.
    Return default if the file does not exist.
    '''
    path = os.path.join(ds, 'metadata.{}.json'.format(key))
    if os.path.exists(path):
        with open(path) as fh:
            return json.load(fh)
    else:
        return default


def _setDataInFile(ds, key, data):
    '''
    Serialize and save data to the file indexed by key.
    '''
    path = os.path.join(ds, 'metadata.{}.json'.format(key))
    with open(path, 'w') as fh:
        json.dump(data, fh, indent=0)


def getUniprotRelease(ds):
    release = getData(ds, 'uniprotRelease')
    if not release:
        raise KeyError('uniprotRelease', ds)
    return release


def getSourceUrls(ds):
    '''
    return: a list of source urls.
    '''
    return getData(ds, 'sources', [])

        
def getReleaseName(ds):
    '''
    returns: the name of this roundup release, in human readable form.  e.g. 2
    '''
    return getDatasetId(ds)


def getReleaseDate(ds):
    '''
    returns: a datetime.date object
    '''
    # logging.debug('getReleaseDate: ds={}'.format(ds))
    dateStr = getData(ds, 'releaseDate')
    # logging.debug('getReleaseDate: dateStr={}'.format(dateStr))
    return datetime.datetime.strptime(dateStr, "%Y-%m-%d").date()


def set_release_date(ds, date=None):
    '''
    date: a datetime.date object.  if None, datetime.date.today() is used.
    '''
    if date is None:
        date = datetime.date.today()
    setData(ds, 'releaseDate', str(date.strftime("%Y-%m-%d")))


def getGenomeToName(ds):
    return getData(ds, 'genomeToName', {})


def setGenomeToName(ds, genomeToName):
    return setData(ds, 'genomeToName', genomeToName)


def getGenomeToTaxon(ds):
    return getData(ds, 'genomeToTaxon', {})


def setGenomeToTaxon(ds, genomeToTaxon):
    return setData(ds, 'genomeToTaxon', genomeToTaxon)


def getGenomeToCount(ds):
    return getData(ds, 'genomeToCount', {})


def getDatasetStats(ds):
    '''
    return: a dict of 'numGenomes', 'numPairs', and 'numOrthologs'
    '''
    return {'numGenomes': getData(ds, 'numGenomes'), 
            'numPairs': getData(ds, 'numPairs'), 
            'numOrthologs': getData(ds, 'numOrthologs')}


def getGenomeToGenes(ds):
    return getData(ds, GENOME_TO_GENES, {})


def getGeneToName(ds):
    return getData(ds, GENE_TO_NAME, {})


def getGeneToGenome(ds):
    return getData(ds, GENE_TO_GENOME, {})


def getGeneToGoTerms(ds):
    return getData(ds, GENE_TO_GO_TERMS, {})


def getGeneToGeneIds(ds):
    return getData(ds, GENE_TO_GENE_IDS, {})


def getTermToData(ds):
    return getData(ds, TERM_TO_DATA, {})


def getTaxonToData(ds):
    return getData(ds, TAXON_TO_DATA, {})


#######
# DONES
#######

# "done" functions use the mysql database to track which jobs are done.
# Tracking jobs is useful to avoid rerunning jobs that are already done.
# pros: concurrency. fast even with millions of dones.
# cons: different mysql db for dev and prod, so must use the prod code on a prod dataset.

def getDones(ds):
    ns = 'roundup_dataset_{}_dones'.format(getDatasetId(ds))
    return dones.get(ns)


def reset_blast_dones(ds, query_genome, subject_genome, reverse=False):
    '''
    Used from the command line to fix incorrectly marked blast dones.
    '''

    if reverse:
        getDones(ds).unmark(('blast', subject_genome, query_genome))
    else:
        getDones(ds).unmark(('blast', query_genome, subject_genome))


#######
# STATS
#######

STATS_CACHE = {}


def stats_ns(ds):
    '''
    DRY function for creating the stats namespace from the dataset name.
    '''
    return 'roundup_dataset_{}_stats'.format(getDatasetId(ds))


def getStats(ds):
    '''
    Return a Stats object with a connection context manager that proactively
    closes connections to the database.
    '''
    if ds not in STATS_CACHE:
        connect = kvstore.make_closing_connect(config.openDbConn)
        kv = kvstore.KVStore(connect, ns=stats_ns(ds))
        STATS_CACHE[ds] = Stats(kv)

    return STATS_CACHE[ds]


@contextlib.contextmanager
def statsCM(ds):
    '''
    A context manager that returns a Stats object and closes the database
    connection used by the stats objecdt when exiting the context.  

    This is useful when rapidly getting or putting stats, which would overwhelm
    the database if each action opened and closed a database connection.
    '''
    reuser = None
    try:
        connect = kvstore.make_reusing_connect(config.openDbConn)
        kv = kvstore.KVStore(connect, ns=stats_ns(ds))
        stats = Stats(kv)
        yield stats
    finally:
        if reuser:
            reuser.close()


class Stats(object):
    '''
    Thin wrapper around a key-value store that stores performance stats for
    blast and "rsd" jobs.
    '''
    def __init__(self, kv):
        '''
        kv: a KVStore object used to store the stats.
        '''
        self.kv = kv
        self.ready = False

    def _get_kv(self):
        if not self.ready:
            self.kv.create() # will create the table if it does not exist
            self.ready = True

        return self.kv

    def clean(self):
        '''
        Remove all stored stats. Useful for resetting the stats or cleaning
        up when done storing them.
        '''
        self._get_kv().drop()
        self.ready = False # _get_kv() will create table next time.

    def get(self, key):
        '''
        returns: a dict of stats.  if key is not present, returns an empty dict
        '''
        return self._get_kv().get(key, default={})

    def put(self, key, stats):
        '''
        stats: a dict of statistics for blast, rsd, or a pair.
        '''
        self._get_kv().put(key, stats)

    def putBlast(self, qdb, sdb, startTime, endTime):
        stats = {'type': 'blast', 'qdb': qdb, 'sdb': sdb, 'startTime': startTime, 'endTime': endTime}
        key = ('blast', qdb, sdb)
        self.put(key, stats)
        return stats

    def getBlast(self, qdb, sdb):
        key = ('blast', qdb, sdb)
        return self.get(key)

    def putRsd(self, qdb, sdb, divEvalues, startTime, endTime):
        stats = {'type': 'roundup', 'qdb': qdb, 'sdb': sdb, 'divEvalues': divEvalues, 'startTime': startTime, 'endTime': endTime}
        key = ('rsd', qdb, sdb)
        self.put(key, stats)
        return stats

    def getRsd(self, qdb, sdb):
        key = ('rsd', qdb, sdb)
        return self.get(key)


##########
# Workflow for constructing a dataset

def workflow(ds, previous_dataset=None):
    '''
    This function runs the entire workflow needed to create a new roundup dataset,
    from preparing the directories, to downloading genomes, to preprocessing,
    to computing orthologs, and to post-processing.
    '''
    dsid = getDatasetId(ds)
    ns = 'roundup_dataset_{}_workflow'.format(dsid)

    def do(name, func, *args, **kws):
        task = lsfdo.FuncTask(name, func, args, kws)
        lsfdo.do(ns, task)

    do('prepare_dataset', prepare_dataset, ds)
    do('download_sources', download_sources, ds)
    do('process_sources', process_sources, ds)

    # Extract some metadata
    do('extract_taxon', extract_taxon_data, ds)
    do('extract_gene_ontology', extract_gene_ontology_data, ds)

    # Scan dat files, compiling the counts of various genomes, completes, reference, sprot, etc.  This can take an hour or so.
    do('examine_dats', examine_dats, ds)

    # Some output from the examine_dats function showing that trembl 50-100 times bigger than sprot
    # td23@clarinet002-039:/groups/cbi/sites/roundup/datasets/4/code/app$     cd $DS_APP && time $DS_PYTHON roundup/dataset.py examine_dats(ds)
    # Namespace(action='examine_dats', dataset='/groups/cbi/sites/roundup/datasets/4', ds_func=<function examineDats at 0x25e5e60>, func=<function dispatch_ds_func at 0x25ecb18>)
    # dats: ['/groups/cbi/sites/roundup/datasets/4/sources/uniprot/uniprot_sprot.dat', '/groups/cbi/sites/roundup/datasets/4/sources/uniprot/uniprot_trembl.dat']
    # surprisesFile: /groups/cbi/sites/roundup/datasets/4/dat_surprises.txt
    # countsFile: /groups/cbi/sites/roundup/datasets/4/dat_genome_counts.txt
    # gathering ids in /groups/cbi/sites/roundup/datasets/4/sources/uniprot/uniprot_sprot.dat... 2012-12-14 14:26:19.210954
    # entry count: 100000
    # entry count: 200000
    # entry count: 300000
    # entry count: 400000
    # entry count: 500000
    # gathering ids in /groups/cbi/sites/roundup/datasets/4/sources/uniprot/uniprot_trembl.dat... 2012-12-14 14:27:37.548696
    # entry count: 100000
    # entry count: 200000
    # entry count: 300000
    # ...
    # entry count: 28200000
    # entry count: 28300000


    # Set the genomes list to only contain genomes with
    # enough sequences marked 'Complete proteome' and that are a Eukaryota, Bacteria,
    # or Archaea, and have taxon data.  Historically some viruses have been missing
    # taxon data, which means that the NCBI taxon id Uniprot sets for them is not
    # present in the taxon data we get from NCBI.  Weird, right?
    do('set_genomes_from_filter', set_genomes_from_filter, ds)

    # Here is output showing the taxons with missing taxon data (two big ones!),
    # and the viruses that got filtered out:

    # td23@clarinet002-039:/groups/cbi/sites/roundup/datasets/4/code/app$     cd $DS_APP && time $DS_PYTHON roundup/dataset.py set_genomes_from_filter(ds)
    # Namespace(action='set_genomes_from_filter', dataset='/groups/cbi/sites/roundup/datasets/4', ds_func=<function set_genomes_from_filter at 0x2428050>, func=<function dispatch_ds_func at 0x242ac08>)
    # all_genomes 30540
    # after_size_filter 2072
    # missing_taxon 1094619 complete_count 25721 name Phytophthora sojae (strain P6497)
    # missing_taxon 1054147 complete_count 12152 name Dictyostelium fasciculatum (strain SH3)
    # missing_taxon 587202 complete_count 202 name Variola virus (isolate Human/Japan/Yamada MS-2(A)/1946)
    # missing_taxon 587203 complete_count 211 name Variola virus (isolate Human/Brazil/v66-39/1966)
    # missing_taxon 587201 complete_count 201 name Variola virus (isolate Human/South Africa/102/1965)
    # after_missing_taxon_filter 2067
    # filtered_taxon_cat_code 10665 cat_code V complete_count 550 name Enterobacteria phage T4 (Bacteriophage T4)
    # filtered_taxon_cat_code 654913 cat_code V complete_count 531 name White spot syndrome virus (isolate Shrimp/China/Tongan/1996)
    # filtered_taxon_cat_code 176652 cat_code V complete_count 468 name Invertebrate iridescent virus 6 (IIV-6)
    # filtered_taxon_cat_code 654926 cat_code V complete_count 231 name Ectocarpus siliculosus virus 1 (isolate New Zealand/Kaikoura/1988)
    # filtered_taxon_cat_code 654925 cat_code V complete_count 472 name Emiliania huxleyi virus 86 (isolate United Kingdom/English Channel/1999)
    # filtered_taxon_cat_code 75320 cat_code V complete_count 381 name Vibrio phage KVP40 (Bacteriophage KVP40)
    # filtered_taxon_cat_code 10760 cat_code V complete_count 244 name Enterobacteria phage T7 (Bacteriophage T7)
    # filtered_taxon_cat_code 10685 cat_code V complete_count 204 name Bacillus phage SP01 (Bacteriophage SP01)
    # filtered_taxon_cat_code 28321 cat_code V complete_count 278 name Amsacta moorei entomopoxvirus (AmEPV)
    # filtered_taxon_cat_code 10506 cat_code V complete_count 793 name Paramecium bursaria Chlorella virus 1 (PBCV-1)
    # filtered_taxon_cat_code 928301 cat_code V complete_count 251 name Fowlpox virus (strain NVSL)
    # filtered_taxon_cat_code 10847 cat_code V complete_count 289 name Enterobacteria phage phiX174 (Bacteriophage phi-X174)
    # filtered_taxon_cat_code 205877 cat_code V complete_count 225 name Mycobacterium phage Bxz1 (Mycobacteriophage Bxz1)
    # filtered_taxon_cat_code 45406 cat_code V complete_count 270 name Enterobacteria phage RB32 (Bacteriophage RB32)
    # filtered_taxon_cat_code 10249 cat_code V complete_count 257 name Vaccinia virus (strain Copenhagen)
    # filtered_taxon_cat_code 212035 cat_code V complete_count 909 name Acanthamoeba polyphaga mimivirus (APMV)
    # filtered_taxon_cat_code 10693 cat_code V complete_count 273 name Enterobacteria phage RB51 (Bacteriophage RB51)
    # filtered_taxon_cat_code 10726 cat_code V complete_count 255 name Enterobacteria phage T5 (Bacteriophage T5)
    # filtered_taxon_cat_code 442493 cat_code V complete_count 220 name Enterococcus phage phiEF24C (Enterococcus bacteriophage phi-EF24C)
    # filtered_taxon_cat_code 169683 cat_code V complete_count 306 name Pseudomonas phage phiKZ.
    # filtered_taxon_cat_code 66711 cat_code V complete_count 281 name Enterobacteria phage AR1 (Bacteriophage AR1)
    # filtered_taxon_cat_code 12353 cat_code V complete_count 273 name Enterobacteria phage RB69 (Bacteriophage RB69)
    # filtered_taxon_cat_code 10254 cat_code V complete_count 217 name Vaccinia virus (strain Western Reserve)
    # after_taxon_category_filter 2044

    do('make_genome_to_taxon', make_genome_to_taxon, ds)
    do('make_genome_to_name', make_genome_to_name, ds)

    # Compile genome fasta files from the dat files.  This can take a while
    do('extract_from_dats', extract_from_dats, ds)

    # Format the genomes for BLAST.  This also can take a while.  By default it
    # distributes the jobs onto orchestra, otherwise it takes a really long time.
    do('format_genomes', format_genomes, ds)

    # Prepare jobs for computing all orthologs
    do('prepare_jobs', prepare_jobs, ds)

    # Compute the orthologs by running lots of jobs on Orchestra.  This step is often
    # run several times, depending on how many times orchestra services melt down and
    # how often my code is messed up.

    # use my mysql creds, since web user does not have alter table perms.
    # export ROUNDUP_MYSQL_CREDS_FROM_CNF=True
    # export DS_DIR=/groups/cbi/sites/roundup/datasets/4
    # export DS_APP="$DS_DIR/code/app"
    # export DS_PYTHON="$DS_DIR/code/venv/bin/python"
    # echo $DS_DIR $DS_APP $DS_PYTHON
    do('compute_jobs', compute_jobs, ds)

    # Make the Change Log for this dataset / roundup release vs. the previous release (3)
    if previous_dataset:
        do('make_change_log', make_change_log, ds, [previous_dataset])

    # When computation is finished, extract stats and prepare files for download:
    do('extract_dataset_stats', extract_dataset_stats, ds)
    do('extract_performance_stats', extract_performance_stats, ds)
    # making all genomes and orthologs available for download.
    do('process_genomes_for_download', process_genomes_for_download, ds)
    do('collate_orthologs', collate_orthologs, ds)
    # compress genomes and ortholog download files once they are ready.  This takes a long time.  Could be parallelized.
    do('zip_download_paths', zip_download_paths, ds)


def main():
    parser = argparse.ArgumentParser(description='')
    subparsers = parser.add_subparsers(help='')

    def add_ds_parser(name, func, help=None):
        '''
        Create a subparser named by name and add a dataset argument.
        Also add a function which takes 'ds' as its first argument.
        Return the subparser so more arguments can be added if needed.
        '''
        subparser = subparsers.add_parser(name, help=help)
        subparser.add_argument('ds', metavar='dataset', 
                               help='root directory of the dataset')
        subparser.set_defaults(func=func)
        return subparser

    subparser = add_ds_parser('workflow', workflow)
    subparser.add_argument('--previous-dataset', help='''The dataset directory
                           of the previous dataset, used to create a log of
                           changes between this dataset and the previous
                           one.''')

    # convert_to_orthoxml
    subparser = add_ds_parser('convert_to_orthoxml', convert_to_orthoxml,
                              help='Convert dataset orthologs to orthoxml')
    subparser.add_argument('--origin', required=True, help='The source of the orthologs. e.g. roundup.')
    subparser.add_argument('--origin-version', required=True, help='The version of the orthologs. e.g. 3.0')
    subparser.add_argument('--database', required=True, help='The database name of the source of genomes. e.g. Uniprot')
    subparser.add_argument('--database-version', required=True, help='The version of the source genomes. e.g. 2012_04')

    # make changelog
    subparser = add_ds_parser('make_change_log', make_change_log,
        help='Create list of differences between two datasets.')
    subparser.add_argument('others', metavar='previous_dataset', 
        nargs='+', help='previous datasets to compare with dataset.')

    # parse command line args and pass as keyword args to func.
    args = parser.parse_args()
    kws = dict(vars(args))
    del kws['func']
    return args.func(**kws)


if __name__ == '__main__':
    main()


########################
# DEPRECATED / UNUSED
########################

